
# Visual Text Detection using Transformers

Visual Text Detection refers to the process of identifying and extracting text from images or videos, which falls under the subset of Optical Character Recognition (OCR). OCR traditionally involves converting printed, scripted, or handwritten text into machine-readable text. Text can be in any form, meaning the background can be complex, including cluttered or noisy scenes and high-contrast backgrounds, making it hard to distinguish from a machine perspective. Visual Text Detection using Transformers leverages the power of advanced deep learning architectures to identify and extract text from images and videos. Transformers, known for their self-attention mechanisms and parallel processing capabilities, excel in handling complex visual data. By employing models such as Vision Transformers (ViTs) and combining them with Optical Character Recognition (OCR) techniques, this approach achieves high accuracy in various real-world applications, including document digitization, automated license plate recognition, and assistive technologies for the visually impaired.

## Contributions

In this project, I reviewed and studied multiple research projects pertaining to visual text detection systems, specifically utilizing Transformers. I assessed different approaches, identified key novelties, and tried to identify drawbacks compared to other studies' approaches. I tried to understand codes of few SOTA solutions, and performed inferences based on the given instructions on test images.

## Papers Reviewed: 

[Vision Transformer for Fast and Efficient Scene
Text Recognition]([https://arxiv.org/pdf/2105.08582])

This paper introduces ViTSTR, a vision transformer-based model for scene text recognition (STR) that focuses on acheiving a balance between accuracy, speed, and efficiency. It utilizes efficient vision transformers like DeiT to acheive state-of-the-art results in STR tasks. ViTSTR utilizes a simplified single-stage model architecture based on vision transformers, focusing on extracting multiple feature vectors for text prediction and achieving efficient text recognition. 

**Model Architecture**: The model architecture includes encoder blocks with Layer Normalization, Multi-head Self-Attention, Multilayer Perceptron, and linear projections for word prediction to generate machine readable text. Atienza has performed ananlysis with multiple degree of augmentations of the input data including RandAugment and transformations like distortions, rotation, streching, and image compression. ViTSTR emphasizes on performance penalty by optimizing model architecture and parametrs to acheive accuracy improvement, without significant time drop during inference.  The Vision Transformer (ViT) model architecture is similar to the original transformer model designed for NLP tasks by Vaswani et al., but only utilizes the encoder part. Instead of using word embeddings, input images are reshaped into a sequence of flattened 2D patches. Each image of dimension H × W with C channels is divided into patches of dimension P × P, resulting in a patch sequence of length N. The transformer encoder uses a constant width D for embeddings and features in all its layers. Each flattened patch is converted to an embedding of size D via linear projection. A learnable class embedding of dimension D is prepended to the sequence, and a unique position encoding of the same dimension is added to each embedding. In ViTSTR, a learnable position encoding is used. Unlike the original ViT, which uses the output vector corresponding to the learnable class embedding for object category prediction, ViTSTR extracts multiple feature vectors from the encoder, corresponding to the maximum length of text plus two tokens ([GO] and [s]). The [GO] token marks the beginning of text prediction, and [s] indicates the end or a space, repeated up to the maximum sequence length. Each input goes through Layer Normalization (LN) and the Multi-head Self-Attention layer (MSA), which determines the relationships between feature vectors. The MSA uses multiple heads to attend to different representation subspaces at different positions. The Multilayer Perceptron (MLP) performs feature extraction with GELU activation, and residual connections are used between the output of LN and MSA/MLP. The input to the encoder is a combination of the class embedding, patch embeddings, and position encodings. The MSA block processes the input with residual connections, followed by the MLP block. Finally, the head of the model forms the word prediction through a sequence of linear projections for each token up to the maximum text length plus two.

**Challenges in STRs Tasks**: The paper discussed common failure cases in STR, including confusion b/w similar symbols, scripted fonts, unknown text styles, glare, vertical text, curved text images, and occluded symbols, which signifies the importance of semantics in resolving such ambiguous situations.

**Summarized Result**: The experimental evaluation in the paper focuses on comparing different strong baseline Scene Text Recognition (STR) methods using a framework developed by Baek et al. The study emphasizes the importance of consistent train and test datasets to ensure fair performance evaluations across different models. Various strong baseline models such as CRNN, R2AM, GCRNN, Rosetta, RARE, STAR-Net, and TRBA were reproduced and trained multiple times with different random seeds for a fair comparison with ViTSTR. Performance metrics such as accuracy, speed, number of parameters, and FLOPS were reported to provide a comprehensive overview of the trade-offs among different models. For instance, transitioning from tiny to base ViTSTR leads to additional ~80M parameters to achieve a 3.4% increase in accuracy. Comparisons between different models like RARE and TRBA highlight the trade-offs between accuracy, speed, and computational efficiency. TRBA achieves an accuracy of 84.3%, while ViTSTR achieves competitive accuracy at higher speeds with fewer parameters and FLOPS. ViTSTR configurations are designed to maximize accuracy, speed, and computational efficiency simultaneously, positioning them at the forefront of trade-offs in scene text recognition tasks. The experimental results underscore the significance of not only accuracy but also speed and computational efficiency in the development of effective STR models for real-world applications.


[Pure Transformer with Integrated Experts for
Scene Text Recognition](https://arxiv.org/pdf/2211.04963)

**Introduction**: The main motive of this paper is to propose a novel approach in Scene Text Recognition (STR) by introducing a *transformer-only* model called Pure Transformer with Integrated Experts (PTIE). The solution aims to address the limitations of existing methods in STR by leveraging the transformer architecture to capture long-term dependencies effectively in scene text images. The paper explore the potential of utilizing transformers at an earlier stage in the model to enhance STR performance, as transformers have shown strong capabilities in capturing long-term dependencies, which are crucial in recognizing scene text. By introducing PTIE, the research aims to overcome existing models that use hybrid CNN-transformer architectures, demonstrating the effectiveness of a transformer-only approach in scene text recognition. The study mainly focus on two key areas for improvement in existing methods: firstly, addressing the issue of low prediction accuracy for the first decoded character in STR tasks, and secondly, investigating how images with different original aspect ratios respond to patch resolutions, especially in comparison to the fixed patch resolution employed by vision transformers (ViT). The primary goal of the experiments were to demonstrate that PTIE, as a transformer model capable of processing multiple patch resolutions and decoding in both original and reverse character orders, can achieve state-of-the-art results in seven commonly used benchmarks for scene text recognition.

**Impact of patch resolution**: The authors discussed the impact of patch resolution on the encoder in Scene Text Recognition (STR) models, focusing the need to understand how different resolutions affect performance. They note that text images from natural scenes vary in size and aspect ratio, influencing encoder effectiveness. Thus, Resizing images to fixed dimensions is crucial for standardizing input data. Preserving original resolutions with padding can decrease performance. Vision Transformer (ViT) models process resized images by dividing them into patches before encoding. Understanding the effects of patch resolution is vital for optimizing text data processing in STR models, aiming to improve overall performance.

**Architecture**: The methodology section introduces the baseline model, featuring a Vision Transformer (ViT) encoder and transformer decoder enhanced by PTIE, a novel transformer with integrated experts. Each expert, Exp $i$, $j$, uses image patches of resolution $r_i$ and ground-truth texts of type $j$, with patch sizes of $4$ × $8$ and $8$ × $4$. The image patches are processed through linear layers, summed with positional encodings, and passed through the encoder and decoder to generate class probabilities. Cross-entropy loss is applied to these probabilities. The model integrates all experts with shared parameters and processes four input sets per sample but uses only one in the baseline model. It maintains latency close to the baseline while requiring only a quarter of the parameters compared to standard ensembles. For positional encoding, the model combines positional encoding with input tokens using positional attention during multi-head attention, following (another) a particular approach. Positional attentions for the encoder and decoder are computed using specific linear layers, with all layers sharing the positional attentions. PTIE is designed as a transformer-only model capable of processing multiple patch resolutions and decoding text in both original and reverse character orders. PTIE differs from traditional ensemble methods by generating four predictions per input sample while requiring only a quarter of the parameters and inference time, thus maintaining competitive accuracy. The model integrates experts, selecting final predictions based on word probability, which enhances prediction accuracy and streamlines the process. PTIE has been evaluated across seven benchmarks and compared with over 20 state-of-the-art methods, demonstrating superior performance and achieving state-of-the-art results in most benchmarks.

**Results**: PTIE was evaluated on a range of datasets, including synthetic ones like MJSynth and SynthText, and real ones such as IIIT 5K-Words, ICDAR 2013 and 2015, Street View Text, CUTE80, and COCO-Text. PTIE performed better in 6 out of 7 benchmarks, surpassing other methods by up to 2.9% in accuracy, while PTIE–Vanilla performed best in 5 benchmarks. Fine-tuning with COCO-Text improved performance, with PTIE–Untied showing a 0.4% higher weighted average over PTIE–Vanilla. Ablation studies revealed that transformer-only models outperformed hybrid CNN-transformer models, indicating that pure transformer models can achieve competitive results. Ensemble models like Ensemble–Diverse reached an accuracy of 92.4% but required significantly more parameters (183.2M), where as PTIE–Diverse achieved the same accuracy with only 45.9M parameters. Different patch resolutions and ground-truth types were crucial, with PTIE–Identical showing a 1.4% accuracy drop compared to PTIE–Diverse. PTIE–Diverse had a latency of 52ms, which is comparable to baseline models and much lower than Ensemble–Diverse’s 202ms. PTIE's latency of 11ms per cropped scene text image makes it suitable for real-time applications, emphasizing accuracy over latency in specific contexts. PTIE models, especially PTIE–Untied, demonstrate significant advancements in scene text recognition, offering high accuracy with fewer parameters, thus showcasing efficiency in both performance and resource usage.

**Drawbacks (as stated in paper)**: The authors of the PTIE model identified that while it achieved state-of-the-art results across most benchmarks, it fell short of reaching the peak of current performance standards. This indicates that there is still potential for enhancing the model’s accuracy to surpass existing benchmarks. Apart from that, a notable drawback is the lower accuracy of the first decoded character in the PTIE model. Improving the accuracy of this initial character prediction is crucial, as it could boost the overall performance of the model by a great margin. 


[TEXT RECOGNITION IN IMAGES BASED ON TRANSFORMER WITH HIERARCHICAL
ATTENTION](https://ieeexplore.ieee.org/document/8803203)

**Introduction**: This research paper proposes a novel deep neural network structure known as the Hierarchical Attention Transformer Network (HATN) for accurate and efficient recognition of regular and irregular scene text. The main contributions of HATN include the application of the Transformer structure in text recognition, eliminating the need for RNN units, thereby enhancing context representation and significantly reducing training time. Additionally, the paper introduces a two-dimensional attention mechanism to extract context information from images and a hierarchical attention scheme to expedite training and inference procedures, catering to the unique characteristics of images containing texts.

**Architecture**: The architecture consists of two main modules: a ResNet50 backbone and a hierarchical transformer network. The ResNet50 processes the input image into feature vectors while maintaining a two-dimensional structure, preserving spatial information crucial for text recognition, especially for irregular text orientations. The hierarchical transformer network includes encoder and decoder subnets, each with self-attention and feedforward layers. The decoder additionally contains encoder-decoder-attention layers. A hierarchical attention mechanism addresses different context levels (character, word, sentence) by progressively expanding receptive fields through the encoder blocks. This approach reduces computational costs while enhancing context representation. The network leverages a multi-head attention mechanism to increase representative power, dividing queries, keys, and values into multiple heads for parallel processing. Image preprocessing involves text direction normalization and height normalization to standardize inputs. The ADAM optimizer is used for training, and the network is tailored for a character set comprising digits, alphabets, and an end signal.

**Efficiency**: The attention mechanism in traditional transformer architectures can be computationally expensive, particularly when dealing with the global context information of images. The hierarchical attention mechanism proposed by the authors addresses this by employing smaller receptive fields in the lower attention blocks and gradually increasing the field size, thereby balancing computational efficiency with representation power.

**Results**: The proposed method was evaluated through extensive experiments on various public datasets, including regular datasets (SVT, IIIT5K, ICDAR2003, ICDAR2013) and irregular datasets (CUTE80, SVT-Perspective, ICDAR2015). Pretrained on 8 million synthetic images, the model was fine-tuned on relevant training sets. Ablation studies showed that the 2D structure outperformed the 1D structure, especially on irregular datasets, highlighting the importance of 2D spatial information. The hierarchical attention mechanism reduced training costs while maintaining high performance. In comparison with state-of-the-art approaches, the proposed HATN model achieved top results on IIIT5K and IC13, and performed well on irregular datasets, demonstrating its capability to handle complex text arrangements. Efficiency comparisons revealed that HATN required the least training and inference time, making it both accurate and computationally efficient.

[DTrOCR: Decoder-only Transformer for Optical Character Recognition](https://openaccess.thecvf.com/content/WACV2024/papers/Fujitake_DTrOCR_Decoder-Only_Transformer_for_Optical_Character_Recognition_WACV_2024_paper.pdf)

**Introduction**: The key novelty of DTrOCR, a novel text recognition method that employs a decoder-only Transformer architecture for Optical Character Recognition (OCR). This approach deviates from the traditional encoder-decoder structure commonly used in text recognition/localization tasks, focusing solely on the decoder component. DTrOCR leverages a generative language model pre-trained on a large corpus, adapting successful techniques from natural language processing to text recognition in computer vision. By concentrating exclusively on the decoder, DTrOCR demonstrates superior performance compared to state-of-the-art methods, significantly improving text recognition for various types of text, including printed, handwritten, and scene text in both English and Chinese. Results showed DTrOCR's enhanced efficiency and effectiveness through extensive experiments, highlighting its potential to simplify and advance text recognition processes.

**Architecture**: The proposed DTrOCR model employs a novel text recognition method using a Decoder-only Transformer architecture, consisting of patch embedding and Transformer decoder modules. The input image is resized and divided into fixed-size patches, which are then transformed into vectors with positional encoding added to preserve spatial information. These sequences are fed into the Transformer decoder, which uses a generative language model pre-trained on a large corpus. The decoder starts with a special token [SEP] to separate image and text sequences and predicts the first word token. It continues to generate subsequent tokens in an autoregressive manner until it reaches the end-of-sequence token [EOS], producing the final text output. The model leverages the multi-head self-attention mechanism and a feed-forward network in the decoder, simplifying the design by eliminating the need for an encoder. Pre-training on synthetic datasets fuses the model with both language and image knowledge, while fine-tuning on real-world datasets addresses specific text recognition challenges, ensuring robust performance. During inference, the same process is followed to generate predicted tokens iteratively until the text is fully recognized.

**Results**: The experiments in the paper focus on evaluating the proposed method for scene image, printed, and handwritten text recognition in both English and Chinese. The method was pre-trained using synthetic datasets created from large text corpora, including PILE and CC100 for English and Chinese, respectively. Synthetic text images were generated using open-source libraries like SynthTIGER and TRDG, ensuring a distribution across scene, printed, and handwritten text. Fine-tuning was performed using real-world datasets such as COCO-Text and ICDAR 2013. The models were tested on various benchmarks including ICDAR 2013, Street View Text, and IIIT5K-Words, and evaluated for metrics like word accuracy, precision, recall, and F1 score. Results showed that the proposed method, DTrOCR, achieved high accuracy and robustness, outperforming several state-of-the-art methods on standard benchmarks, particularly excelling in recognizing occluded and irregularly arranged text scenes, as well as multiple lines of text.

**Ananlysis**: 
The detailed analysis of the proposed method's effectiveness in English Scene Text Recognition (STR) and Chinese Text Recognition (CTR) revealed key insights. Using real datasets, the architecture analysis compared various model configurations, highlighting that model (a) with an image-specific encoder achieved higher accuracy in CTR, while the proposed method excelled in STR. This indicates that sophisticated features are crucial for Chinese characters, yet a decoder-only structure can still achieve sufficient accuracy. The GPT-based decoder outperformed RoBERTa, suggesting that an encoder-decoder architecture is not always necessary. Training process analysis showed significant improvements with pre-trained models, data augmentation, and fine-tuning with real datasets. The effect of pre-training datasets demonstrated that increased data variations are more critical than the number of training iterations, with larger pre-training datasets resulting in higher accuracy. Additionally, model size analysis indicated that larger models achieved better accuracy, confirming the importance of a powerful language model for text recognition tasks.

PARSeq

***
I attempted to understant and infer the code of [ViTSTR](https://github.com/roatienza/deep-text-recognition-benchmark), and [TrOCR](https://github.com/microsoft/unilm/tree/master/trocr).














